---
title: "mda_report"
author: "Pengwei Xu"
date: "2024-12-10"
output: pdf_document
---

# Method

## Method 1: Penalty Ridge Regression

```{=tex}
\begin{itemize}
\item[]\textrm{We try to fit the dataset by using Penalty Ridge Regression. Ridge Regression, also known as L2 regularization, is a regularization technique used to prevent overfitting in linear regression models. In overfitting, a model is excessively complex and performs well on the training data but poorly on unseen test data. Ridge Regression addresses this issue by adding a penalty equivalent to the square of the magnitude of the coefficients to the loss function.}
\end{itemize}
```
### Definition: Penalty Ridge Regression

$$
\begin{aligned}
y_i=f(x_i)+\epsilon_i, \text{where }&\epsilon_i\sim N(0,\sigma^2)\\
&f\in C_2[a,b]
\end{aligned}
$$

### Choosing of f(x)

$$
\text{We consider using a cubic spline function:}\\
\begin{aligned}
f(x)=\sum^{m+4}_{j=1} B_{j,4}(x)\beta_j
\end{aligned}
$$

### Estimate of beta by Penalized Least Squares

$$
\hat{\beta}=argmin\sum^n_{i=1}(y_i-f(x_i))^2+\lambda\int_{a}^{b} \{f^{''}(x)\}^2 \,dx \
$$

### Theorem 1

$$
\begin{aligned}
&\text{Penalized Ridge Regression: }\\
&\hat{\beta}=argmin\sum^n_{i=1}(y_i-f(x_i))^2+\lambda\beta^TS\beta
\end{aligned}
$$

# Result
```{r,echo=FALSE}
library(dplyr)

# Read in dataset
data = read.csv("https://raw.githubusercontent.com/Panta-Rhei-LZ/MDA_9159_Team_Bits_Project/main/Team_Bits_Data.csv")

# Remove price=0 entries
data = data[data$PRICE != 0, ]

# Remove rows with NA in all columns except 'YR_RMDL'
# will impute the missing values, if any, for this variable
data = data %>% dplyr::filter(!if_any(-YR_RMDL, is.na))

# Remove redundant columns 
# as "_D" columns are merely descriptions of their corresponding columns
columns_to_remove <- c("SSL", "OBJECTID", "GIS_LAST_MOD_DTTM", "QUALIFIED", "SALE_NUM", 
                       "BLDG_NUM", "STYLE_D", "STRUCT_D", "GRADE_D", "CNDTN_D", 
                       "EXTWALL_D", "ROOF_D", "INTWALL_D", "USECODE", "HEAT_D", 
                       "NUM_UNITS", "STRUCT")

data <- data[, !(names(data) %in% columns_to_remove)]
missing_data = round(sapply(data, function(x) mean(is.na(x) * 100)), 3)
library(lubridate)
library(fastDummies)

# Transform Yes/No for having AC to numerical values
data$AC = ifelse(data$AC == 'Y', 1, 0)

# Create dummy variables for categorical predictors
data = dummy_cols(
  data, 
  select_columns = c("HEAT", "STYLE", "GRADE", "CNDTN", 
                     "EXTWALL", "ROOF", "INTWALL", "AC"), 
  remove_selected_columns = TRUE,
  remove_first_dummy = TRUE
)

# Add SALEYEAR
data$SALE_YEAR = year(ymd_hms(data$SALEDATE))

# Add SALEYEAR and AYB diff
data$SALE_AYB_DIFF = data$SALE_YEAR - data$AYB

# Add SALEYEAR and EYB diff
data$SALE_EYB_DIFF = data$SALE_YEAR - data$EYB

# Add SALEYEAR and YR_RMDL diff
data$SALE_RMDL_DIFF = data$SALE_YEAR - data$YR_RMDL

# Convert SALEDATE column to numeric values
data$SALEDATE = as.numeric(as.Date(data$SALEDATE))
# Replace NA with column median
data = data.frame(lapply(data, function(column) {
  column_median = median(column, na.rm = TRUE)
  column[is.na(column)] = column_median
  column
}))
# Define box-cox and inverse box-cox transformation
powerfun = function(y, lambda) {
  if (lambda == 0) {
    return(log(y))
  } else {
    return((y^lambda - 1) / lambda)
  }
}

inv_powerfun = function(y_transformed, lambda) {
  if (lambda == 0) {
    return(exp(y_transformed))
  } else {
    return((lambda * y_transformed + 1)^(1/lambda))
  }
}
set.seed(9159)

# Randomly sample 600 data entries for our project
clean_data = data[sample(nrow(data), 600),]

data_train = clean_data[1:500, ]  # First 500 rows for training
data_valid = clean_data[501:600, ]  # Last 100 rows for validation
lambda = 0.35
```

## Model Training without Non-Linear Predictors
```{r,echo=FALSE}

library(glmnet)

X_train = model.matrix(powerfun(PRICE, lambda)~., data=data_train)[,-1]
X_test = model.matrix(powerfun(PRICE, lambda)~., data=data_valid)[,-1]
y_train = powerfun(data_train$PRICE, lambda)
y_test = powerfun(data_valid$PRICE, lambda)


cv_ridge = cv.glmnet(X_train, y_train, alpha=0)
best_ridge_lambda = cv_ridge$lambda.min



ridge_model = glmnet(X_train, y_train, alpha=0, lambda=best_ridge_lambda)




predict(ridge_model, type="coefficients", s=best_ridge_lambda)



pred_train = predict(ridge_model, newx=X_train, s=best_ridge_lambda)


SSE = sum((y_train - pred_train)^2)
SST = sum((y_train - mean(y_train))^2)


R2 = 1 - SSE / SST

n_train = length(y_train)
p_train = sum(coef(ridge_model, s=best_ridge_lambda) != 0)
Adjusted_R2 = 1 - (1 - R2) * (n_train) / (n_train - p_train)

cat("R-squared:", R2, '\n')
cat("Adjusted R-squared:", Adjusted_R2, '\n')

```


```{r}
# Predicted values are transformed by powerfun(PRICE, lambda)
valid_pred = predict(ridge_model, s=best_ridge_lambda, newx=X_test)

# Using inv_powerfun to convert back to original scale
inv_valid_pred = inv_powerfun(valid_pred, lambda)


### Compute MSE, RMSE and RMSLE


# Compute metrics on validation dataset
mse = mean((data_valid$PRICE - inv_valid_pred)^2)
rmse = sqrt(mse)
rmsle = sqrt(mean((log(data_valid$PRICE) - log(inv_valid_pred))^2))



cat("MSE:", mse, '\n')
cat("RMSE:", rmse, '\n')
cat("RMSLE:", rmsle, '\n')
```
```{r,echo=FALSE}
X_train2 = model.matrix(
  powerfun(PRICE, lambda)~. + poly(BATHRM, 4) + 
    poly(HF_BATHRM, 3) + poly(BEDRM, 3) + poly(EYB, 2), 
  data=data_train
)[,-1]
X_test2 = model.matrix(
  powerfun(PRICE, lambda)~. + poly(BATHRM, 4) + 
    poly(HF_BATHRM, 3) + poly(BEDRM, 3) + poly(EYB, 2), 
  data=data_valid
)[,-1]



cv_ridge2 = cv.glmnet(X_train2, y_train, alpha=0)
best_ridge_lambda2 = cv_ridge2$lambda.min



ridge_model2 = glmnet(X_train2, y_train, alpha=0, lambda=best_ridge_lambda2)



# Check non-zero LASSO coefficients
predict(ridge_model2, type="coefficients", s=best_ridge_lambda2)

# Predicted values on training data
pred_train = predict(ridge_model2, newx=X_train2, s=best_ridge_lambda2)

# Compute training SSE and SST
SSE = sum((y_train - pred_train)^2)
SST = sum((y_train - mean(y_train))^2)

# R-squared
R2 = 1 - SSE / SST

# Adjusted R-squared
n_train = length(y_train)
p_train = sum(coef(ridge_model2, s=best_ridge_lambda2) != 0)
Adjusted_R2 = 1 - (1 - R2) * (n_train) / (n_train - p_train)

cat("R-squared:", R2, '\n')
cat("Adjusted R-squared:", Adjusted_R2, '\n')

```

```{r,echo=FALSE}
# Predicted values are transformed by powerfun(PRICE, lambda)
valid_pred = predict(ridge_model2, s=best_ridge_lambda2, newx=X_test2)

# Using inv_powerfun to convert back to original scale
inv_valid_pred = inv_powerfun(valid_pred, lambda)
# Compute metrics on validation dataset
mse = mean((data_valid$PRICE - inv_valid_pred)^2)
rmse = sqrt(mse)
rmsle = sqrt(mean((log(data_valid$PRICE) - log(inv_valid_pred))^2))
cat("MSE:", mse, '\n')
cat("RMSE:", rmse, '\n')
cat("RMSLE:", rmsle, '\n')
```
Noted that the adjusted R-squared is approximately 0.8108437, indicating that the model is quite excellent. Regardless of whether non-linear predictors are included, the R-squared does not change significantly. However, adding non-linear predictors causes an increase in MSE, though this is still acceptable. In my opinion, the full model is a highly suitable choice.